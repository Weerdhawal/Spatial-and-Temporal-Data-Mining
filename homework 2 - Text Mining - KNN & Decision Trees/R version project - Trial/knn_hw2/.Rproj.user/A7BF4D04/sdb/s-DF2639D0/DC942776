{
    "collab_server" : "",
    "contents" : "#install read text adn quanteda\n\n\nlibrary(ggplot2)\nlibrary(reshape2)\nrequire(tm)\nrequire(SnowballC)\nrequire(reshape2)\nrequire(ggplot2)\nlibrary(tm)\nlibrary(gplots)\nlibrary(RColorBrewer)\nlibrary(wordcloud)\nlibrary(proxy)\n\n#load the corpus\nall <- Corpus(DirSource(\"I:\\\\Masters\\\\SPRING 18\\\\SPATIAL AND TEMPORAL\\\\spatial\\\\homework1\\\\20news-18828\\\\20news-18828\\\\\",\n                        encoding=\"UTF-8\",recursive=TRUE),readerControl=list(reader=readPlain,language=\"en\"))\n\n#check values\nall[[1]]\n\n#lower case\nall<- tm_map(all, tolower)\n#remove punctuations\nall<-tm_map(all,removePunctuation)\n#strip extra white spaces\nall <- tm_map(all,stripWhitespace)\n#apply Numbers from files\nall<-tm_map(all,removeNumbers)\n\n#now remove the standard list of stopwords, like you've already worked out\nall.nostopwords <- tm_map(all, removeWords, stopwords(kind = \"en\"))\n#remove words specified in the document\nall.nostopwords<-tm_map(all.nostopwords,removeWords,c(\"on\",\"in\",\"next to\",\"infront of\",\"behind\",\"between\",\"under\",\"through\",\n                                                      \" around\",\"i\",\"me\",\"my\",\"mine\",\"you\",\"your\",\"yours\",\"he\",\"him\",\"his\",\"she\",\n                                                      \"her\",\"hers\",\"it\",\"its\",\"we\",\"us\",\"our\",\"ours\",\"they\",\"their\",\"theirs\",\"them\",\"easily\",\n                                                      \"loudly\",\"quickly\",\"quietly\",\"sadly\",\"silently\",\"slowly\",\"always\",\"frequently\",\"often\",\"once\"))\n\n#making TF matrix\ntdm<-TermDocumentMatrix(all.nostopwords,control =list(weighting= weightTf,normalize = TRUE))\n\n#REMOVE SPARSE TERMS AND CONVERT TO MATRIX\nfinal_tdm<- removeSparseTerms(tdm, sparse = 0.99)\nfinal_matrix<- as.matrix(final_tdm)\n#TRANSPOSE MATRIX FOR ROWS=ARTICLE , COLUMN=TERM i.e to get Document term matrix\nfsb<-t(final_matrix)\n#FEATURE SELECTION\n#SORT VALUES to get top 100 words\nfsb_s<-sort(colSums(fsb),decreasing = TRUE)\nfsb_d<-data.frame(word=names(fsb_s),freq=fsb_s)\ntop100<-head(fsb_d,100)\n#create word cloud \nwordcloud(words=names(fsb_s),freq=fsb_s,min.freq=1000,random.order=F)\ncolna<-names(fsb_s)\n\n\n#\ncolna<-findFreqTerms(final_tdm, lowfreq = 2284,highfreq = 20334)\na<-data.frame(fsb)\ntop100_sel<-a[,colna] \n#top100_sel1<-as.matrix(top100_sel)\ntop100_sel1<-as.matrix(fsb)\ntop100_sel1<- top100_sel1[1000:2000,]\n\n#histogram\nbarplot(top100[1:100,]$freq, las = 2, names.arg = top100[1:100,]$word,col =\"lightblue\", main =\"Most Frequent Words\",ylab = \"Word frequencies\")\n\n\n#cALCULATING SIMILARITIES\neuc_dist<-dist(top100_sel1,method = \"euclidean\")\nmelted_eud_d<-melt(as.matrix(euc_dist))\nggplot(data = melted_eud_d, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile()+ scale_fill_gradient(low = \"yellow\", high = \"red\")\n\ncos_dist<-dist(top100_sel1,method = \"cosine\")\nmelted_cos_d<-melt(as.matrix(cos_dist))\nggplot(data = melted_cos_d, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile()+ scale_fill_gradient(low = \"yellow\", high = \"red\")\n\njac_dist<-dist(top100_sel1,method = \"jaccard\")\nmelted_jac_d<-melt(as.matrix(jac_dist))\nggplot(data = melted_jac_d, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile()+ scale_fill_gradient(low = \"yellow\", high = \"red\")\n\n#setp 7\n\n#similarities\ncos_euc<-cor(euc_dist,cos_dist,method = \"pearson\")\neuc_jac<-cor(jac_dist,euc_dist,method = \"pearson\")\njac_cos<-cor(cos_dist,jac_dist,method = \"pearson\")\n\n#linear regression\nvec_cos<-as.vector(head(cos_dist,500))\nvec_euc<-as.vector(head(euc_dist,500))\nvec_jac<-as.vector(head(jac_dist,500))\ndf_top100<-data.frame(head(top100_sel1,500))\nlr_cos_euc<-lm(vec_cos~vec_euc,df_top100)\nlr_euc_jac<-lm(vec_euc~vec_jac,df_top100)\nlr_jac_cos<-lm(vec_jac~vec_cos,df_top100)\n\n\n#plot for Simialiteries \nscatter.smooth(x=vec_euc,y=vec_cos,pch=21,col=\"blue\",lpars =list(col = \"red\", lwd = 3, lty = 3),xlim=c(0,20),xlab=\"Euclidean\",ylab=\"Cosine\")\n\nscatter.smooth(x=vec_cos,y=vec_jac,pch=21,col=\"blue\",lpars =list(col = \"red\", lwd = 3, lty = 3),xlim=c(0,1),xlab=\"Cosine\",ylab=\"Jaccard\")\n\nscatter.smooth(x=vec_jac,y=vec_euc,pch=21,col=\"blue\",lpars =list(col = \"red\", lwd = 3, lty = 3),xlim=c(0,1),ylim = c(0,30),xlab=\"Jaccard\",ylab=\"Euclidean\")\n\n\n\n\n\n\n#step 9\ntrial_melted_cos <- melted_cos_d\ntail(trial_melted_cos[order(trial_melted_cos$value),],10)\ntrial_melted_jac <- melted_jac_d\ntail(trial_melted_jac[order(trial_melted_jac$value),],10)\ntrial_melted_euc <- melted_eud_d\ntail(trial_melted_euc[order(trial_melted_euc$value),],10)\n\n#convert to csv\nwrite.csv(top100_sel1,\"top100Features.csv\")\n\nfsb<-read.csv(\"input.csv\")\n\n\n\n\n\n\n\n",
    "created" : 1519269846378.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1777840595",
    "id" : "DC942776",
    "lastKnownWriteTime" : 1519256497,
    "last_content_update" : 1519256497,
    "path" : "I:/Masters/SPRING 18/SPATIAL AND TEMPORAL/spatial/homework1/HW1_preprocess.r",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}